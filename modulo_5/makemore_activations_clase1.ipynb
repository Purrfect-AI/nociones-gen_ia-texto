{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e313c42-fd40-41c4-83cd-11365e1975f0",
   "metadata": {},
   "source": [
    "# Makermore - MPL y las capas de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6bf741-5985-41a2-9031-1462d64a8f81",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "* Clase anterior: Modelo de redes neuronales de Bengio et al. 2003 para modelos de lenguajes a nivel de caracteres\n",
    "* La idea es ir hacia modelos más complejos, como Recurrent Neural Networks, ResNet, WaveNet y eventualmente, modelos basados en mecanismos de atención como los Generative Pretrained Transformers (GPTs).\n",
    "* Estos modelos, aunque muy expresivos, presentan problemas que los investigadores fueron resolviendo con tiempo y esfuerzo.\n",
    "  * Modelos muy profundos son difíciles de entrenar (Por qué?)\n",
    "* La idea de la clase es\n",
    "  * Optimizar algunas fases del entrenamiento. Kaiming Init.\n",
    "  * Ganar una intuición sobre el comportamiento de las activaciones de la red y sobre todo, con los gradientes en la etapa de backpropagation. Visualizar las salidas y la información de los gradientes.\n",
    "  * Pytorchificar el código.\n",
    "  * Entender por que no es posible entrenar redes muuuuuuuy grandes o profundas.\n",
    "* Batch Normalization\n",
    "* ResNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c39465-53f2-49c5-bea8-75a6c38c6490",
   "metadata": {},
   "source": [
    "## Código inicial..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f23747-a725-48d8-84d3-fb09aa77d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb70f895-003d-498d-9edc-4ea7b76b438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import idna\n",
    "\n",
    "# def utf8_to_punycode(text: str) -> str:\n",
    "#     \"\"\"Encodes a UTF-8 string to its Punycode representation.\"\"\"\n",
    "#     return idna.encode(text).decode('ascii')\n",
    "\n",
    "def punyencode(text: str) -> str:\n",
    "    \"\"\"Encodes a UTF-8 string to its Punycode representation, handling spaces by encoding each word separately.\"\"\"\n",
    "    \n",
    "    return \" \".join([idna.encode(word).decode('ascii') for word in text.split()])\n",
    "    \n",
    "def punydecode(punycode: str) -> str:\n",
    "    \"\"\"Decodes a Punycode string back to UTF-8.\"\"\"\n",
    "    #return idna.decode(punycode)\n",
    "    return \" \".join([idna.decode(word) for word in punycode.split()])\n",
    "\n",
    "def process_name(name):\n",
    "    name = name.lower()\n",
    "    for n in name.split():\n",
    "        if len(n) < 2:\n",
    "            return ''\n",
    "    try:\n",
    "        return punyencode(name)\n",
    "    except:\n",
    "        #print(f'Cant convert {name}')\n",
    "        return ''\n",
    "\n",
    "dataset = open(\"data/city_names_full.txt\", 'r').read().split('\\n')\n",
    "with open('data/city_names_puny.txt', 'w') as f:\n",
    "    for n in dataset:\n",
    "        name = process_name(n)\n",
    "        if name != '':\n",
    "            f.write(name+'\\n')\n",
    "dataset = open(\"data/city_names_puny.txt\", 'r').read().split('\\n')\n",
    "puny = [x for x in dataset if 'xn--' in x]\n",
    "nopuny = [x for x in dataset if 'xn--' not in x]\n",
    "np.random.seed(42)\n",
    "dataset = [x.item() for x in np.random.choice(nopuny, 100000,replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b89d9-73e1-4f7b-b042-843ec3799162",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = ['*'] + sorted(list(set([y for x in dataset for y in x])))\n",
    "ctoi = {c:i for i, c in enumerate(charset)}\n",
    "itoc = {i:c for i, c in enumerate(charset)}\n",
    "charset_size = len(charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be47df-b962-4828-85ad-6552562ae97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, charset_size, context_size, emb_size, hidden_size, g=torch.Generator().manual_seed(42)):\n",
    "        self.charset_size = charset_size\n",
    "        self.context_size = context_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.C = torch.randn(self.charset_size, self.emb_size, generator=g)\n",
    "        self.W1 = torch.randn(self.emb_size*self.context_size, self.hidden_size, generator=g)\n",
    "        self.b1 = torch.randn(self.hidden_size, generator=g)\n",
    "        self.W2 = torch.randn(self.hidden_size, self.charset_size, generator=g)\n",
    "        self.b2 = torch.randn(self.charset_size, generator=g)\n",
    "        self.parameters = [self.C, self.W1, self.b1, self.W2, self.b2]\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.emb = self.C[x]\n",
    "        self.embcat = self.emb.view(-1, self.emb_size*self.context_size)\n",
    "        self.preact = self.embcat @ self.W1 + self.b1\n",
    "        self.act = torch.tanh(self.preact)\n",
    "        self.logits = self.act @ self.W2 + self.b2\n",
    "        return self.logits\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum([p.nelement() for p in self.parameters])\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = None\n",
    "\n",
    "    def requieres_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.requieres_grad = True\n",
    " \n",
    "    def sample(self, nsamples, g=torch.Generator().manual_seed(42)):\n",
    "        samples = []\n",
    "        for n in range(nsamples):\n",
    "            context = [0] * context_size\n",
    "            out = []\n",
    "            while True:\n",
    "                logits = self(torch.tensor(context))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "                context = context[1:] + [idx]\n",
    "                out.append(itoc[idx])\n",
    "                if idx == 0:\n",
    "                    samples.append(''.join(out[:-1]))\n",
    "                    break\n",
    "        return samples\n",
    "\n",
    "def nll(logits, Y):\n",
    "    return F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83723b-61a7-481a-8869-1c14f34843c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset: list):\n",
    "    X, Y  = [], []\n",
    "    for d in dataset:\n",
    "        example = list(d) + ['*']\n",
    "        context = [0] * context_size\n",
    "        for c in example:\n",
    "            X.append(context)\n",
    "            Y.append(ctoi[c])\n",
    "            context = context[1:] + [ctoi[c]] \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "# build the dataset\n",
    "context_size = 3\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(dataset)\n",
    "n1 = int(.8 * len(dataset))  # límite para el 80% del dataset\n",
    "n2 = int(.9 * len(dataset))  # límite para el 90% del dataset\n",
    "Xtr, Ytr = build_dataset(dataset[:n1])    # 80%\n",
    "Xva, Yva = build_dataset(dataset[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(dataset[n2:])    # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3e220-d7bc-4010-853c-5d8f207c55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 10\n",
    "n_hidden = 256\n",
    "model = Model(charset_size, context_size, emb_size, n_hidden, torch.Generator().manual_seed(42))\n",
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37018b-f2d1-4e9e-8f72-1f10c484edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "steps = 200000\n",
    "losses = []\n",
    "for i in range(steps):\n",
    "    # 1. Forwards pass\n",
    "    idx = torch.randint(0, len(Xtr), (batch_size, ))\n",
    "    logits = model(Xtr[idx])\n",
    "    \n",
    "    # 2. loss\n",
    "    loss = nll(logits, Ytr[idx])\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 3. zero grad\n",
    "    for p in model.parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # 4. backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # learing rate decay\n",
    "    for p in model.parameters:\n",
    "        p.data -= p.grad * lr\n",
    "    if i%10000 == 0:\n",
    "        print(f'epoch {i}/{steps} loss: {loss.item():0.4f}')\n",
    "    #break    \n",
    "print(f'Train loss {nll(model(Xtr), Ytr).item()}')\n",
    "print(f'Vtion loss {nll(model(Xva), Yva).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d114696-f6d9-4f79-90df-07ecd9c5ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0cbff-2521-4cb5-846e-0868896587a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e689304-5ba1-484a-971a-f445f0ada281",
   "metadata": {},
   "source": [
    "## Inicialización de la red para mejorar el loss en los primeros ciclos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebdb726-69d7-4b9a-a4ff-5b727dbb70bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ff4f846-f73b-45cd-b1a5-ec04d8bba427",
   "metadata": {},
   "source": [
    "## Visualizando la saturación de la tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27452812-e731-4f48-8fed-c50ffe42fb1e",
   "metadata": {},
   "source": [
    "```python\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (np.e ** (2*x) - 1)/(np.e ** (2*x) + 1)\n",
    "        out = Value(t, _children=(self, ), _op=\"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4835c4b-1b12-48c5-9051-9e6bba79e912",
   "metadata": {},
   "source": [
    "## Arreglando la saturación de la tanh con Kaiming Init\n",
    "De donde vienen esos números mágicos que multiplican a los pesos Ws y bs?\n",
    "\n",
    "https://arxiv.org/abs/1502.01852\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.init.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98040de6-4f8f-458e-a969-4bcb1f1dfe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13588064-868a-4f9b-b0b9-156f1f1ad1f0",
   "metadata": {},
   "source": [
    "## Agregando capas al modelo (PyTorchificando el código)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a763d5-d1b2-4e8e-9530-62eb4dcfe31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(model.layers[-3].out.abs() > .97, cmap='grey', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f7609-b074-4673-8d94-ffb9ec2b8dea",
   "metadata": {},
   "source": [
    "## Visualizando estadísticas de las capas de activación y los gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb685a-b718-46c0-9a04-71e4ddf11ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "969f7b46-fd04-454f-a603-b589b5691c51",
   "metadata": {},
   "source": [
    "# MLPs con Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618baa47-6767-490c-8d04-f734ddc5d026",
   "metadata": {},
   "source": [
    "## Volviendo a nuestro modelo sin modularizar\n",
    "Para entender en más profundidad los conceptos relacionados con Batch Normalization, volvamos a nuestro modelo sin pytorchificar.\n",
    "\n",
    "Si queremos que la entrada de nuestra función de activación reciba valores con distribución $\\mathbb{N}(0, 1)$, bien podríamos simplemente normalizar `preact`. Como todas las operaciones son diferenciables, es posible hacerlo sin muchos problemas 🤯\n",
    "\n",
    "https://arxiv.org/pdf/1502.03167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2191c657-7244-4e62-96c0-dbc922523362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595a889e-10ee-4a82-b8fc-4f85aad21bd6",
   "metadata": {},
   "source": [
    "## Pytorchificando la Batch Norm Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a56929-ca23-46a3-b173-1ab2836cb548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2784b403-d54c-4947-b7bb-6e851b203da8",
   "metadata": {},
   "source": [
    "# ResNet Walkthtough\n",
    "* https://arxiv.org/pdf/1512.03385\n",
    "* https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac05ec7-4efd-44e2-964f-5e59bae3a239",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "* Las redes con muchas capas son difíciles de entrenar debido a los problemas del desvanecimiento y explosión de los gradientes.\n",
    "* La correcta inicialización de la red, no garantiza que los gradientes se desvanezcan o exploten durante la etapa de entrenamiento.\n",
    "* Cuanto más profunda la red, más dificil de resolver es el problema.\n",
    "* La normalización por lotes previene estos problemas, normalizando la salida de la capa lineal para no saturar las capa Tanh que sigue.\n",
    "* La normalización por lotes trae aparejado un costo.\n",
    "  * Los ejemplos de un batch de entrenamiento quedan asociados matemáticamente.\n",
    "  * La activación para un ejemplo tiene *jitter*. Vibra dependiendo de los otros ejemplos del batch. Esto resulta ser bueno para el entrenamiento, pero no deja de ser raro y puede tener efectos indeseados y difíciles de debuguear, si uno no sabe bien lo que está pasando. \n",
    "  * El modelo deja de poder procesar elementos individuales.\n",
    "  * Para esto se debe o bien agregar una etapa de calibración para calcular la mean y std del training set o calcular la running mean y std durante el entrenamiento, agregando dos modos posibles a la capa de BatchNorm (training o inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd603eff-f706-4a4b-8433-b6327b45350b",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "* Comprobar que el mean y std del training set es compatible con el running mean y std calculado durante el entrenemiento de la capa de BatchNorm1d\n",
    "* Reimplementar el modelo usando las funciones `nn.Linear()` y `nn.BatchNorm1d()` de PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fff26-f269-4cd4-b943-3927f4b570d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tallerLLMs]",
   "language": "python",
   "name": "conda-env-tallerLLMs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
